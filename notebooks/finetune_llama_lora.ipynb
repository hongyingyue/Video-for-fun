{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff30e7a8",
   "metadata": {},
   "source": [
    "# Tutorial on instruction tuning of LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e38227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "\n",
    "!pip install transformers --quiet\n",
    "!pip install peft --quiet\n",
    "!pip install datasets --quiet\n",
    "!pip install deepspeed --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModelForCasualLM, AutoTokenizer, HfArgumentParser, TrainingArguments\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0770418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45b8b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SftTrainingArguments(TrainingArguments):\n",
    "    model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, MyTrainingArguments))\n",
    "\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,  # if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],)\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce4a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "\n",
    "if model_args.config_name:\n",
    "    config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "else:\n",
    "    raise ValueError(\"config_name or model_name_or_path\")\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(f\"Adding pad token {DEFAULT_PAD_TOKEN}\")\n",
    "    tokenizer.add_special_tokens(dict(pad_token=DEFAULT_PAD_TOKEN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70167f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "train_dataset = None\n",
    "eval_dataset = None\n",
    "\n",
    "if training_args.do_train:\n",
    "    with training_args.main_process_first(desc=\"loading and tokenization\"):\n",
    "        path = Path(data_args.dataset_dir)\n",
    "        files = [os.path.join(path,file.name) for file in path.glob(\"*.json\")]\n",
    "        logger.info(f\"Training files: {' '.join(files)}\")\n",
    "        train_dataset = build_instruction_dataset(\n",
    "            data_path=files,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=data_args.max_seq_length,\n",
    "            data_cache_dir = None,\n",
    "            preprocessing_num_workers = data_args.preprocessing_num_workers)\n",
    "    logger.info(f\"Num train_samples  {len(train_dataset)}\")\n",
    "    logger.info(\"training example:\")\n",
    "    logger.info(tokenizer.decode(train_dataset[0]['input_ids']))\n",
    "if training_args.do_eval:\n",
    "    with training_args.main_process_first(desc=\"loading and tokenization\"):\n",
    "        files = [data_args.validation_file]\n",
    "        logger.info(f\"Evaluation files: {' '.join(files)}\")\n",
    "        eval_dataset = build_instruction_dataset(\n",
    "            data_path=files,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=data_args.max_seq_length,\n",
    "            data_cache_dir = None,\n",
    "            preprocessing_num_workers = data_args.preprocessing_num_workers)\n",
    "    logger.info(f\"Num eval_samples  {len(eval_dataset)}\")\n",
    "    logger.info(\"eval example:\")\n",
    "    logger.info(tokenizer.decode(eval_dataset[0]['input_ids']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
